#!/var/remote/software/python/2.7.3/bin/python
"""
Launcher for DisplayCluster
Usage: ./startdisplaycluster --help
"""

import os
import sys
import xml.etree.ElementTree as ET
import subprocess
import shlex
import distutils.spawn
import argparse
import socket

parser = argparse.ArgumentParser()
parser.add_argument("--config", help="The configuration file to load")
parser.add_argument("--session", help="The session to load")
parser.add_argument("--printcmd", help="Print the command without executing it",
                    action="store_true")
parser.add_argument("--vglrun", help="Run the main application using vglrun",
                    action="store_true")
args = parser.parse_args()


# Let's get a few paths set

# DisplayCluster directory; this is the parent directory of this script
# Ideally, this should be set when you do module load DisplayCluster/*

# This was the old way of setting the path
# DC_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

if 'DISPLAYCLUSTER_DIR' not in os.environ:
    print("ERROR: $DISPLAYCLUSTER_DIR environment variable not set")
    exit(-1)
else:
    DC_PATH = os.environ['DISPLAYCLUSTER_DIR']

DC_BIN = DC_PATH + '/bin/displaycluster'

if args.config:
    DC_CONFIG_FILE = args.config
else:
    fqhostname = socket.gethostname()
    try:
        hostname = fqhostname[:fqhostname.index('.')]
    except ValueError:
        hostname = fqhostname
    if hostname == 'vis-cubes-fe':
        DC_CONFIG_FILE = '/var/remote/software/DisplayCluster/configs/configuration-vis-cubes-ipaddr-v0.4.xml'
    else:
        DC_CONFIG_FILE = '/var/remote/software/DisplayCluster/configs/configuration-z2-ipaddr-v0.4.vx.separate.xml'
        #DC_CONFIG_FILE = DC_PATH + '/share/DisplayCluster/examples/configuration.xml'


# set the Python path so the pydc module can be found
if 'PYTHONPATH' not in os.environ:
    os.environ['PYTHONPATH'] = DC_PATH + '/python'
else:
    os.environ['PYTHONPATH'] += os.pathsep + DC_PATH + '/python'

# for example scripts
os.environ['PYTHONPATH'] += os.pathsep + DC_PATH + '/examples'

# add in the default Python path provided by the Python interpreter since it
# is not provided in our GUI Python console
os.environ['PYTHONPATH'] += os.pathsep + os.pathsep.join(sys.path)

# add our own lib folder
if 'LD_LIBRARY_PATH' not in os.environ:
    os.environ['LD_LIBRARY_PATH'] = DC_PATH + '/lib'
else:
    os.environ['LD_LIBRARY_PATH'] += os.pathsep + DC_PATH + '/lib'

# find full path to mpiexec; if MPI is installed in a non-standard location the
# full path may be necessary to launch correctly across the cluster.
MPIRUN_CMD = distutils.spawn.find_executable('mpiexec')

if MPIRUN_CMD is None:
    print('Error, could not find mpiexec executable in PATH')
    exit(-3)

# mpiexec has a different commandline syntax for MVAPICH, MPICH2, and OpenMPI
IS_MVAPICH = distutils.spawn.find_executable('mpiname')
# IS_MPICH2 = distutils.spawn.find_executable('mpich2version')
# arshadsu: find mpich version
IS_MPICH = distutils.spawn.find_executable('mpichversion')
IS_MPICH2 = False
IS_MPICH3 = False
if IS_MPICH:
    result = subprocess.Popen(["mpirun", "--version"], stdout=subprocess.PIPE).communicate()[0]
    ver_line = result.split('\n')[1]
    words = ver_line.split(' ')
    words = [x for x in words if x]
    ver = words[1].split('.')
    IS_MPICH2 = ver[0] == '2'
    IS_MPICH3 = ver[0] == '3'


if IS_MVAPICH:
    MPI_SPECIAL_FLAGS = '-env MV2_ENABLE_AFFINITY 0 -env IPATH_NO_CPUAFFINITY 1'
    EXPORT_DISPLAY = ' -env DISPLAY '
    MPI_ARGS = ' -genvlist MPIEXEC_SIGNAL_PROPAGATION,LD_LIBRARY_PATH -hosts '
elif IS_MPICH2:
    MPI_SPECIAL_FLAGS = '-env MV2_ENABLE_AFFINITY 0 -env IPATH_NO_CPUAFFINITY 1'
    MPI_PER_NODE_HOST = '-host'
    EXPORT_DISPLAY = '-env DISPLAY '
    MPI_ARGS = '-genvlist LD_LIBRARY_PATH'
elif IS_MPICH3:
    MPI_SPECIAL_FLAGS = '-env MV2_ENABLE_AFFINITY 0 -env IPATH_NO_CPUAFFINITY 1'
    MPI_PER_NODE_HOST = ''
    EXPORT_DISPLAY = '-env DISPLAY '
    MPI_ARGS = '-genvlist LD_LIBRARY_PATH'
else:
    MPI_SPECIAL_FLAGS = ''
    EXPORT_DISPLAY = '-x DISPLAY='
    MPI_ARGS = '-x LD_LIBRARY_PATH -host '


# Form the application parameters list
DC_PARAMS = ' --config ' + DC_CONFIG_FILE
if args.session:
    DC_PARAMS += ' --session ' + args.session

if args.vglrun:
    VGLRUN_BIN = 'vglrun '
else:
    VGLRUN_BIN = ''

# form the MPI host list
hostlist = []

# Form the list of commands to execute
runcommands = []

# configuration.xml gives the rest of the hosts and the displays
try:
    XML_CONFIG = ET.parse(DC_CONFIG_FILE)

    # parse the masterProcess element
    master_elem = XML_CONFIG.find('masterProcess')
    if master_elem is None:
        print("masterProcess not found, using defaults: 'localhost' ':0'")
    else:
        host = master_elem.get("host")
        display = master_elem.get('display')

    if host is None:
        host = 'localhost'

    if display is None:
        display = ':0'

    if MPI_PER_NODE_HOST:
        node_host = '%s %s' % (MPI_PER_NODE_HOST, host)
    else:
        node_host = ''
        hostlist.append(host)

    rcmd = '%s %s %s %s -np 1 %s %s %s' % (MPI_SPECIAL_FLAGS, EXPORT_DISPLAY, display, node_host, VGLRUN_BIN, DC_BIN, DC_PARAMS)
    runcommands.append(rcmd)

    # parse the wall process elements
    for elem in XML_CONFIG.findall('.//process'):
        host = elem.get("host")

        if host is None:
            print('Error, no host attribute in <process> tag.')
            exit(-1)


        display = elem.get('display')
        if display == None:
            display = ':0'

        if MPI_PER_NODE_HOST:
            node_host = '%s %s' % (MPI_PER_NODE_HOST, host)
        else:
            node_host = ''
            hostlist.append(host)

        rcmd = '%s %s %s %s -np 1 %s %s' % (MPI_SPECIAL_FLAGS, EXPORT_DISPLAY, display, node_host, DC_BIN, DC_PARAMS)
        runcommands.append(rcmd)

except Exception as e:
    print("Error processing xml configuration '%s'. (%s)" % (DC_CONFIG_FILE, e))
    exit(-2)

HOST_LIST = ",".join(hostlist)
if IS_MPICH3:
    HOST_LIST = "-hosts " + HOST_LIST
RUN_COMMANDS = ' : '.join(runcommands)

START_CMD = '%s %s %s %s' % (MPIRUN_CMD, MPI_ARGS, HOST_LIST, RUN_COMMANDS)

if args.printcmd:
    print(START_CMD)
else:
    print('launching with command: ', START_CMD)
    subprocess.call(shlex.split(START_CMD))
